lvs <- levels(as.factor(n_data[[1]]))
rw <- nrow(n_data)
means <- list()
for (i in lvs){
n_data[paste0("u_", i)] <- ifelse(n_data[, 1] == i, 1, 0)
}
# -----------------------------------------
#                Null model
#------------------------------------------
means_u <- colMeans(n_data[, grepl("^u_", names(n_data))])
p_u <- means_u
l <- list()
for(i in 1:length(means_u)){
l[[i]] <- means_u[i] * log(p_u[i])
}
l <- rw * sum(unlist(l))
dev_l <- -2 * l
e_l <- exp(l)
nulo <- list(UBarra = means_u, P_u = p_u, LogNulo = l, DevNulo = dev_l, LNulo = e_l)
# -----------------------------------------
#             Complete model
#------------------------------------------
l <- list()
for(i in 1:length(lvs)){
u <- n_data[, grepl("^u_", names(n_data))][i]
l[[i]] <- ifelse(u == 0, 0, u * log(u))
}
l <- sum(unlist(l), na.rm = T)
dev_l <- -2*l
e_l <- exp(l)
completo <- list(LogCompleto = l, DevCompleto = dev_l, LCompleto = e_l)
# -----------------------------------------
#             Saturated model
#------------------------------------------
ff <- count(data, vars = c(names(mf)[-1]))
names(ff)[ncol(ff)] <- c("n")
J <- nrow(ff)
aa <- split(mf,mf[,names(mf)[1]])
bb <- lapply(aa, function(x) count(x, vars = c(colnames(x)[-1])))
for (i in 1:length(bb)) {
names(bb[[i]])[ncol(bb[[i]])] <- c(paste0("z_", names(bb[i]), "_j"))
}
for(i in 1:length(bb)) {
bb[[i]] <- join(bb[[i]], ff, by = names(mf)[-1])
bb[[i]][paste0("p_", names(bb[i]), "_j")] <- bb[[i]][paste0("z_", names(bb[i]), "_j")]/bb[[i]]["n"]
}
tb <- as.data.frame(bb[[1]])
tb <- tb[, c(1:(ncol(tb) - 3), ncol(tb) - 1, ncol(tb) - 2, ncol(tb))]
for(i in bb[-1]){
tb <- join(tb, i, by = c(names(mf)[-1], "n"), type = "full")
}
tb[is.na(tb)] <- 0
nc <- length(names(mf)[-1]) + 2
pos <- 0
l <- numeric(length(bb))
tb <- as.data.frame(tb)
for (i in 1:(length(bb))) {
tb[paste0("l_", names(bb[i]))] <- ifelse(tb[, nc + pos + 1] == 0 | tb[, nc + pos] == 0, 0, tb[, nc + pos] * log(tb[, nc + pos + 1]))
pos <- pos + 2
}
tb["Lp"] <- apply(tb[, grep("^l_", names(tb))], 1, function(x) {
if(0 %in% x){
return(0)
} else{
return(sum(x))
}
})
#l <- sum(l)
#dev_l <- -2 * l
#e_l <- exp(l)
saturado <- list(tabla = tb, niveles = bb, poblaciones = ff, no_poblaciones = J, LogSaturado = l, DevSaturado = dev_l, LSaturado = e_l)
return(list(data = n_data, ModeloCompleto = completo, ModeloNulo = nulo, ModeloSaturado = saturado))
}
m <- glsm(prog ~ ses + write, data = Datos)
View(m)
View(m[["ModeloSaturado"]][["tabla"]])
sum(m$ModeloSaturado$tabla$Lp)
glsm <- function(formula, data) {
xdata <- data
mf <- model.frame(formula = formula, data = xdata)
n_data <- as.data.frame(mf)
lvs <- levels(as.factor(n_data[[1]]))
rw <- nrow(n_data)
means <- list()
for (i in lvs){
n_data[paste0("u_", i)] <- ifelse(n_data[, 1] == i, 1, 0)
}
# -----------------------------------------
#                Null model
#------------------------------------------
means_u <- colMeans(n_data[, grepl("^u_", names(n_data))])
p_u <- means_u
l <- list()
for(i in 1:length(means_u)){
l[[i]] <- means_u[i] * log(p_u[i])
}
l <- rw * sum(unlist(l))
dev_l <- -2 * l
e_l <- exp(l)
nulo <- list(UBarra = means_u, P_u = p_u, LogNulo = l, DevNulo = dev_l, LNulo = e_l)
# -----------------------------------------
#             Complete model
#------------------------------------------
l <- list()
for(i in 1:length(lvs)){
u <- n_data[, grepl("^u_", names(n_data))][i]
l[[i]] <- ifelse(u == 0, 0, u * log(u))
}
l <- sum(unlist(l), na.rm = T)
dev_l <- -2*l
e_l <- exp(l)
completo <- list(LogCompleto = l, DevCompleto = dev_l, LCompleto = e_l)
# -----------------------------------------
#             Saturated model
#------------------------------------------
ff <- count(data, vars = c(names(mf)[-1]))
names(ff)[ncol(ff)] <- c("n")
J <- nrow(ff)
aa <- split(mf,mf[,names(mf)[1]])
bb <- lapply(aa, function(x) count(x, vars = c(colnames(x)[-1])))
for (i in 1:length(bb)) {
names(bb[[i]])[ncol(bb[[i]])] <- c(paste0("z_", names(bb[i]), "_j"))
}
for(i in 1:length(bb)) {
bb[[i]] <- join(bb[[i]], ff, by = names(mf)[-1])
bb[[i]][paste0("p_", names(bb[i]), "_j")] <- bb[[i]][paste0("z_", names(bb[i]), "_j")]/bb[[i]]["n"]
}
tb <- as.data.frame(bb[[1]])
tb <- tb[, c(1:(ncol(tb) - 3), ncol(tb) - 1, ncol(tb) - 2, ncol(tb))]
for(i in bb[-1]){
tb <- join(tb, i, by = c(names(mf)[-1], "n"), type = "full")
}
tb[is.na(tb)] <- 0
nc <- length(names(mf)[-1]) + 2
pos <- 0
l <- numeric(length(bb))
tb <- as.data.frame(tb)
for (i in 1:(length(bb))) {
tb[paste0("l_", names(bb[i]))] <- ifelse(tb[, nc + pos + 1] == 0 | tb[, nc + pos] == 0, 0, tb[, nc + pos] * log(tb[, nc + pos + 1]))
pos <- pos + 2
}
tb["Lp"] <- apply(tb[, grep("^l_", names(tb))], 1, function(x) {
if(0 %in% x){
return(0)
} else{
return(sum(x))
}
})
tb <- tb[, -grep("^l_", names(tb))]
l <- sum(tb$Lp)
dev_l <- -2 * l
e_l <- exp(l)
saturado <- list(tabla = tb, niveles = bb, poblaciones = ff, no_poblaciones = J, LogSaturado = l, DevSaturado = dev_l, LSaturado = e_l)
return(list(data = n_data, ModeloCompleto = completo, ModeloNulo = nulo, ModeloSaturado = saturado))
}
m <- glsm(prog ~ ses + write, data = Datos)
View(m[["ModeloSaturado"]][["tabla"]])
m <- glsm(prog ~ gender + read, data = Datos)
library(plyr)
library(repmis)
source_data("https://github.com/hllinas/DatosPublicos/blob/main/hsbdemo.Rdata?raw=false")
Datos <- hsbdemo
attach(Datos)
logistic.train<- function(train_data, method, lr, verbose){
b0<-rep(1, nrow(train_data))
x<-as.matrix(cbind(b0, train_data[,1:(ncol(train_data)-1)]))
y<- train_data[, ncol(train_data)]
beta<- as.matrix(rep(0.5,ncol(x))); likelihood<-0; epoch<-0 #initiate
beta_all<-NULL
beta_at<-c(1,10,50,100,110,150,180,200,300,500,600,800,1000,
1500,2000,4000,5000,6000,10000) #checkpoints (the epochs at which I will record the betas)
#-----------------------------Gradient Descent---------------------#
if(method=="Gradient"){
while( (likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i]))) #product of all the probability
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
beta <- beta + lr*first_d #updating the parameters for a step toward maximization
#to see inside the steps of learning (irrelevant to the main working algo)
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)}
epoch<- epoch+1
}
}
#--------------Newton second order diff method-------------#
else if(method=="Newton"){
while((likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i])))
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
w<-matrix(0, ncol= nrow(x), nrow = nrow(x)) #initializing p(1-p) diagonal matrix
diag(w)<-p*(1-p)
hessian<- -t(x) %*% w %*% x #hessian matrix
hessian<- diag(ncol(x))-hessian #Levenberg-Marquardt method: Add a scaled identity matrix to avoid singularity issues
k<- solve(hessian) %*% (t(x) %*% (y-p)) #the gradient for newton method
beta <- beta + k #updating the parameters for a step toward maximization
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)} #just to inside the learning
epoch<- epoch+1
}
}
else(break)
beta_all<-cbind(beta_all, beta)
colnames(beta_all)<-c(beta_at[1:(ncol(beta_all)-1)], epoch-1)
mylist<-list(as.matrix(beta), likelihood, beta_all)
names(mylist)<- c("Beta", "likelihood", "Beta_all")
return(mylist)
}
logistic.pred<-function(model, test_data){
test_new<- cbind( rep(1, nrow(test_data)), test_data[,-ncol(test_data)]) #adding 1 to fit the intercept
beta<-as.matrix(model$Beta) #extract the best suiting beta (the beta at final epoch)
beta_all<-model$Beta_all #extract all the betas at different checkpoints
ll<- model$likelihood #extract the highest likelihood obtained
log_odd<-cbind(as.matrix(test_new)) %*% beta #logit(p)
probability<- 1/(1+ exp(-log_odd)) # p=logistic(logit(p))
predicted_label<- ifelse(probability >= 0.5, 1, 0) #discrimination rule
k<-cbind(test_data[,ncol(test_data)], predicted_label) # actual label vs predicted label
colnames(k)<- c("Actual", "Predicted")
k<- as.data.frame(k)
tp<-length(which(k$Actual==1 & k$Predicted==1)) #true positive
tn<-length(which(k$Actual==0 & k$Predicted==0)) #true negative
fp<-length(which(k$Actual==0 & k$Predicted==1)) #false positive
fn<-length(which(k$Actual==1 & k$Predicted==0)) #false negative
cf<-matrix(c(tp, fn, fp, tn), 2, 2, byrow = F) #confusion matrix
rownames(cf)<- c("1", "0")
colnames(cf)<- c("1", "0")
p_list<-list(k, cf, beta, ll, beta_all)
names(p_list)<- c("predticted", "confusion matrix","beta", "liklihood", "Beta_all")
return(p_list)
}
prueba <- Datos[c("prog", "ses", "write")]
prueba$prog <- ifelse(prog == "academic", 0, ifelse(prog == "general", 1, 2))
prueba$ses <- ifelse(ses == "low", 0, ifelse(ses == "middle", 1, 2))
prueba <- rbind(
prueba[which(prueba$prog == 1),],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 0)),
],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 2)),
]
)
prueba <- prueba[sample(1:nrow(prueba)), ]
prueba <- as.data.frame(prueba)
prueba <- lapply(prueba, as.numeric)
prueba <- as.data.frame(prueba)
partition <- sample(c(0, 1), size = nrow(prueba), prob = c(0.8, 0.2), replace = T)
train <- prueba[which(partition == 1), ]
test <- prueba[which(partition == 0), ]
partition <- sample(c(1, 0), size = nrow(prueba), prob = c(0.8, 0.2), replace = T)
train <- prueba[which(partition == 1), ]
test <- prueba[which(partition == 0), ]
View(test)
View(train)
newton_betas <- logistic.train(train, "Newton", 0.01, verbose=T)
gradient_betas <- logistic.train(train, "Gradient", 0.01, verbose=T)
View(gradient_betas)
View(newton_betas)
View(prueba)
library(plyr)
library(repmis)
source_data("https://github.com/hllinas/DatosPublicos/blob/main/hsbdemo.Rdata?raw=false")
Datos <- hsbdemo
attach(Datos)
logistic.train<- function(train_data, method, lr, verbose){
b0<-rep(1, nrow(train_data))
x<-as.matrix(cbind(b0, train_data[,1:(ncol(train_data)-1)]))
y<- train_data[, ncol(train_data)]
beta<- as.matrix(rep(0.5,ncol(x))); likelihood<-0; epoch<-0 #initiate
beta_all<-NULL
beta_at<-c(1,10,50,100,110,150,180,200,300,500,600,800,1000,
1500,2000,4000,5000,6000,10000) #checkpoints (the epochs at which I will record the betas)
#-----------------------------Gradient Descent---------------------#
if(method=="Gradient"){
while( (likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i]))) #product of all the probability
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
beta <- beta + lr*first_d #updating the parameters for a step toward maximization
#to see inside the steps of learning (irrelevant to the main working algo)
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)}
epoch<- epoch+1
}
}
#--------------Newton second order diff method-------------#
else if(method=="Newton"){
while((likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i])))
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
w<-matrix(0, ncol= nrow(x), nrow = nrow(x)) #initializing p(1-p) diagonal matrix
diag(w)<-p*(1-p)
hessian<- -t(x) %*% w %*% x #hessian matrix
hessian<- diag(ncol(x))-hessian #Levenberg-Marquardt method: Add a scaled identity matrix to avoid singularity issues
k<- solve(hessian) %*% (t(x) %*% (y-p)) #the gradient for newton method
beta <- beta + k #updating the parameters for a step toward maximization
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)} #just to inside the learning
epoch<- epoch+1
}
}
else(break)
beta_all<-cbind(beta_all, beta)
colnames(beta_all)<-c(beta_at[1:(ncol(beta_all)-1)], epoch-1)
mylist<-list(as.matrix(beta), likelihood, beta_all)
names(mylist)<- c("Beta", "likelihood", "Beta_all")
return(mylist)
}
logistic.pred<-function(model, test_data){
test_new<- cbind( rep(1, nrow(test_data)), test_data[,-ncol(test_data)]) #adding 1 to fit the intercept
beta<-as.matrix(model$Beta) #extract the best suiting beta (the beta at final epoch)
beta_all<-model$Beta_all #extract all the betas at different checkpoints
ll<- model$likelihood #extract the highest likelihood obtained
log_odd<-cbind(as.matrix(test_new)) %*% beta #logit(p)
probability<- 1/(1+ exp(-log_odd)) # p=logistic(logit(p))
predicted_label<- ifelse(probability >= 0.5, 1, 0) #discrimination rule
k<-cbind(test_data[,ncol(test_data)], predicted_label) # actual label vs predicted label
colnames(k)<- c("Actual", "Predicted")
k<- as.data.frame(k)
tp<-length(which(k$Actual==1 & k$Predicted==1)) #true positive
tn<-length(which(k$Actual==0 & k$Predicted==0)) #true negative
fp<-length(which(k$Actual==0 & k$Predicted==1)) #false positive
fn<-length(which(k$Actual==1 & k$Predicted==0)) #false negative
cf<-matrix(c(tp, fn, fp, tn), 2, 2, byrow = F) #confusion matrix
rownames(cf)<- c("1", "0")
colnames(cf)<- c("1", "0")
p_list<-list(k, cf, beta, ll, beta_all)
names(p_list)<- c("predticted", "confusion matrix","beta", "liklihood", "Beta_all")
return(p_list)
}
prueba <- Datos[c("prog", "ses", "write")]
prueba$prog <- ifelse(prog == "academic", 0, ifelse(prog == "general", 1, 2))
prueba$ses <- ifelse(ses == "low", 0, ifelse(ses == "middle", 1, 2))
prueba <- rbind(
prueba[which(prueba$prog == 1),],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 0)),
],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 2)),
]
)
prueba["u0"] <- ifelse(prueba$prog == 0, 1, 0)
prueba <- prueba[, -prueba$prog]
prueba <- prueba[sample(1:nrow(prueba)), ]
prueba <- as.data.frame(prueba)
prueba <- lapply(prueba, as.numeric)
prueba <- as.data.frame(prueba)
partition <- sample(c(1, 0), size = nrow(prueba), prob = c(0.8, 0.2), replace = T)
train <- prueba[which(partition == 1), ]
test <- prueba[which(partition == 0), ]
newton_betas <- logistic.train(train, "Newton", 0.01, verbose=T)
gradient_betas <- logistic.train(train, "Gradient", 0.01, verbose=T)
View(newton_betas)
View(m)
library(plyr)
library(repmis)
source_data("https://github.com/hllinas/DatosPublicos/blob/main/hsbdemo.Rdata?raw=false")
Datos <- hsbdemo
attach(Datos)
logistic.train<- function(train_data, method, lr, verbose){
b0<-rep(1, nrow(train_data))
x<-as.matrix(cbind(b0, train_data[,1:(ncol(train_data)-1)]))
y<- train_data[, ncol(train_data)]
beta<- as.matrix(rep(0.5,ncol(x))); likelihood<-0; epoch<-0 #initiate
beta_all<-NULL
beta_at<-c(1,10,50,100,110,150,180,200,300,500,600,800,1000,
1500,2000,4000,5000,6000,10000) #checkpoints (the epochs at which I will record the betas)
#-----------------------------Gradient Descent---------------------#
if(method=="Gradient"){
while( (likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i]))) #product of all the probability
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
beta <- beta + lr*first_d #updating the parameters for a step toward maximization
#to see inside the steps of learning (irrelevant to the main working algo)
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)}
epoch<- epoch+1
}
}
#--------------Newton second order diff method-------------#
else if(method=="Newton"){
while((likelihood < 0.95) & (epoch<=35000)){
logit<-x%*%beta #Calculate logit(p) = xβᵀ
p <- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))
# Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)
likelihood<-1
for(i in 1:length(p)){
likelihood <- likelihood*(ifelse( y[i]==1, p[i], (1-p[i])))
}
first_d<-  t(x) %*% (y-p)#first derivative of the likelihood function
w<-matrix(0, ncol= nrow(x), nrow = nrow(x)) #initializing p(1-p) diagonal matrix
diag(w)<-p*(1-p)
hessian<- -t(x) %*% w %*% x #hessian matrix
hessian<- diag(ncol(x))-hessian #Levenberg-Marquardt method: Add a scaled identity matrix to avoid singularity issues
k<- solve(hessian) %*% (t(x) %*% (y-p)) #the gradient for newton method
beta <- beta + k #updating the parameters for a step toward maximization
if(verbose==T){
ifelse(epoch%%200==0,
print(paste0(epoch, "th Epoch",
"---------Likelihood=", round(likelihood,4),
"---------log-likelihood=", round(log(likelihood),4),
collapse = "")), NA)}
if(epoch %in% beta_at){beta_all<-cbind(beta_all, beta)} #just to inside the learning
epoch<- epoch+1
}
}
else(break)
beta_all<-cbind(beta_all, beta)
colnames(beta_all)<-c(beta_at[1:(ncol(beta_all)-1)], epoch-1)
mylist<-list(as.matrix(beta), likelihood, beta_all)
names(mylist)<- c("Beta", "likelihood", "Beta_all")
return(mylist)
}
logistic.pred<-function(model, test_data){
test_new<- cbind( rep(1, nrow(test_data)), test_data[,-ncol(test_data)]) #adding 1 to fit the intercept
beta<-as.matrix(model$Beta) #extract the best suiting beta (the beta at final epoch)
beta_all<-model$Beta_all #extract all the betas at different checkpoints
ll<- model$likelihood #extract the highest likelihood obtained
log_odd<-cbind(as.matrix(test_new)) %*% beta #logit(p)
probability<- 1/(1+ exp(-log_odd)) # p=logistic(logit(p))
predicted_label<- ifelse(probability >= 0.5, 1, 0) #discrimination rule
k<-cbind(test_data[,ncol(test_data)], predicted_label) # actual label vs predicted label
colnames(k)<- c("Actual", "Predicted")
k<- as.data.frame(k)
tp<-length(which(k$Actual==1 & k$Predicted==1)) #true positive
tn<-length(which(k$Actual==0 & k$Predicted==0)) #true negative
fp<-length(which(k$Actual==0 & k$Predicted==1)) #false positive
fn<-length(which(k$Actual==1 & k$Predicted==0)) #false negative
cf<-matrix(c(tp, fn, fp, tn), 2, 2, byrow = F) #confusion matrix
rownames(cf)<- c("1", "0")
colnames(cf)<- c("1", "0")
p_list<-list(k, cf, beta, ll, beta_all)
names(p_list)<- c("predticted", "confusion matrix","beta", "liklihood", "Beta_all")
return(p_list)
}
prueba <- Datos[c("prog", "ses", "write")]
prueba$prog <- ifelse(prog == "academic", 0, ifelse(prog == "general", 1, 2))
prueba$ses <- ifelse(ses == "low", 0, ifelse(ses == "middle", 1, 2))
prueba <- rbind(
prueba[which(prueba$prog == 1),],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 0)),
],
prueba[sample(size=length(which(prueba$prog == 1)),
which(prueba$prog == 2)),
]
)
prueba <- prueba[sample(1:nrow(prueba)), ]
prueba <- as.data.frame(prueba)
prueba <- lapply(prueba, as.numeric)
prueba <- as.data.frame(prueba)
partition <- sample(c(1, 0), size = nrow(prueba), prob = c(0.8, 0.2), replace = T)
train <- prueba[which(partition == 1), ]
test <- prueba[which(partition == 0), ]
newton_betas <- logistic.train(train, "Newton", 0.01, verbose=T)
gradient_betas <- logistic.train(train, "Gradient", 0.01, verbose=T)
newton_test <- logistic.pred(newton_betas, test = test)
gradient_test <- logistic.pred(gradient_betas, test = test)
newton_test$`confusion matrix`
